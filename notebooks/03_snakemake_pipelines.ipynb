{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Pipelines with Snakemake\n",
    "\n",
    "**Reproducible Workflows for Energy System Analysis**\n",
    "\n",
    "This notebook introduces Snakemake, a workflow management system that allows you to create reproducible and scalable data analyses. In energy research, we often need to process large datasets, run multiple scenarios, and generate reports - all in a reproducible manner.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the need for workflow management in research\n",
    "2. Learn the basics of Snakemake syntax and concepts\n",
    "3. Create a complete energy system analysis pipeline\n",
    "4. Understand how to parallelize and scale workflows\n",
    "5. Learn best practices for reproducible research pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Workflow Management?\n",
    "\n",
    "Modern energy research involves complex data processing pipelines:\n",
    "\n",
    "### Common Challenges:\n",
    "- **Multiple Steps**: Data download → Processing → Modeling → Analysis → Visualization\n",
    "- **Multiple Scenarios**: Different policy scenarios, years, regions\n",
    "- **Long Computations**: Some models take hours or days to run\n",
    "- **Reproducibility**: Need to re-run analysis with updated data or parameters\n",
    "- **Collaboration**: Others need to reproduce your results\n",
    "- **Scalability**: Running on different computing resources\n",
    "\n",
    "### Traditional Approach Problems:\n",
    "```bash\n",
    "# Manual workflow - error-prone!\n",
    "python download_data.py\n",
    "python process_data.py scenario1\n",
    "python process_data.py scenario2\n",
    "python run_model.py scenario1\n",
    "python run_model.py scenario2\n",
    "python make_plots.py\n",
    "python generate_report.py\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- No dependency tracking\n",
    "- Manual parameter management  \n",
    "- No parallelization\n",
    "- Difficult to resume if something fails\n",
    "- Hard to reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Snakemake\n",
    "\n",
    "Snakemake is a workflow management system that:\n",
    "- **Tracks dependencies** between files\n",
    "- **Parallelizes** execution automatically\n",
    "- **Resumes** from failures\n",
    "- **Scales** from laptops to clusters\n",
    "- **Ensures reproducibility**\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "#### Rules\n",
    "Define how to create output files from input files\n",
    "\n",
    "#### Wildcards\n",
    "Allow parameterization (like {scenario}, {year})\n",
    "\n",
    "#### DAG (Directed Acyclic Graph)\n",
    "Snakemake automatically builds a dependency graph\n",
    "\n",
    "#### Configuration\n",
    "External config files for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by exploring the Snakemake workflow we've created\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "# Check if we're in the right directory\n",
    "os.chdir('../workflows')\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files in directory:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the Snakefile structure\n",
    "with open('Snakefile', 'r') as f:\n",
    "    snakefile_content = f.read()\n",
    "\n",
    "print(\"First 1000 characters of Snakefile:\")\n",
    "print(snakefile_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the configuration file\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration structure:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {type(value).__name__}\")\n",
    "    if isinstance(value, dict) and len(value) < 10:\n",
    "        for subkey in value.keys():\n",
    "            print(f\"  - {subkey}\")\n",
    "    elif isinstance(value, list) and len(value) < 10:\n",
    "        print(f\"  Values: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Snakemake Syntax\n",
    "\n",
    "### Basic Rule Structure:\n",
    "\n",
    "```python\n",
    "rule rule_name:\n",
    "    input:\n",
    "        \"path/to/input/file\"\n",
    "    output:\n",
    "        \"path/to/output/file\"\n",
    "    params:\n",
    "        parameter=\"value\"\n",
    "    script:\n",
    "        \"scripts/script.py\"\n",
    "```\n",
    "\n",
    "### Wildcards Example:\n",
    "\n",
    "```python\n",
    "rule process_scenario:\n",
    "    input:\n",
    "        \"data/raw/input.csv\"\n",
    "    output:\n",
    "        \"results/processed_{scenario}_{year}.csv\"\n",
    "    params:\n",
    "        scenario=\"{scenario}\",\n",
    "        year=\"{year}\"\n",
    "    script:\n",
    "        \"scripts/process.py\"\n",
    "```\n",
    "\n",
    "This rule can create:\n",
    "- `results/processed_baseline_2025.csv`\n",
    "- `results/processed_high_renewable_2030.csv`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Our Energy System Analysis Pipeline\n",
    "\n",
    "Let's examine the workflow we've created step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our workflow structure\n",
    "try:\n",
    "    # Generate the workflow graph\n",
    "    result = subprocess.run(['snakemake', '--dag'], \n",
    "                          capture_output=True, text=True, cwd='.')\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Workflow DAG generated successfully!\")\n",
    "        print(\"\\nFirst part of the DAG:\")\n",
    "        print(result.stdout[:500] + \"...\")\n",
    "    else:\n",
    "        print(\"Error generating DAG:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Snakemake not installed. Let's examine the workflow structure manually.\")\n",
    "    \n",
    "    # Parse the Snakefile to extract rules\n",
    "    rules = []\n",
    "    with open('Snakefile', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('rule '):\n",
    "                rule_name = line.split()[1].rstrip(':')\n",
    "                rules.append(rule_name)\n",
    "    \n",
    "    print(\"Rules in our workflow:\")\n",
    "    for i, rule in enumerate(rules, 1):\n",
    "        print(f\"{i:2d}. {rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workflow Steps Explained\n",
    "\n",
    "Our energy system analysis pipeline consists of several key steps:\n",
    "\n",
    "### 1. Data Acquisition\n",
    "- Download demand data, renewable profiles, network topology\n",
    "- For this demo, we create realistic synthetic data\n",
    "\n",
    "### 2. Data Processing  \n",
    "- Clean and format data for each year\n",
    "- Aggregate to appropriate temporal/spatial resolution\n",
    "\n",
    "### 3. Network Building\n",
    "- Create PyPSA networks for each scenario and year\n",
    "- Apply scenario-specific parameters\n",
    "\n",
    "### 4. Optimization\n",
    "- Solve the linear programming problem\n",
    "- Find optimal dispatch and capacity expansion\n",
    "\n",
    "### 5. Visualization\n",
    "- Generate plots for each scenario\n",
    "- Maps, time series, generation mix\n",
    "\n",
    "### 6. Analysis\n",
    "- Calculate system metrics\n",
    "- Compare scenarios\n",
    "- Sensitivity analysis\n",
    "\n",
    "### 7. Reporting\n",
    "- Generate final HTML report\n",
    "- Archive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine one of our scripts to understand how Snakemake passes parameters\n",
    "with open('scripts/download_data.py', 'r') as f:\n",
    "    script_content = f.read()\n",
    "\n",
    "print(\"Key parts of the download_data.py script:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract the main function\n",
    "lines = script_content.split('\\n')\n",
    "in_main = False\n",
    "main_content = []\n",
    "\n",
    "for line in lines:\n",
    "    if 'def main():' in line:\n",
    "        in_main = True\n",
    "    elif in_main and line.startswith('def ') and 'main' not in line:\n",
    "        break\n",
    "    elif in_main:\n",
    "        main_content.append(line)\n",
    "\n",
    "print('\\n'.join(main_content[:30]))  # Show first 30 lines of main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Workflow\n",
    "\n",
    "### Basic Snakemake Commands:\n",
    "\n",
    "```bash\n",
    "# Show what would be run\n",
    "snakemake --dry-run\n",
    "\n",
    "# Run the workflow\n",
    "snakemake\n",
    "\n",
    "# Run with 4 parallel jobs\n",
    "snakemake --cores 4\n",
    "\n",
    "# Run specific target\n",
    "snakemake results/summary_report.html\n",
    "\n",
    "# Run specific scenario/year combination\n",
    "snakemake results/plots/generation_mix_baseline_2025.png\n",
    "\n",
    "# Show workflow graph\n",
    "snakemake --dag | dot -Tpng > workflow.png\n",
    "\n",
    "# Force re-run everything\n",
    "snakemake --forceall\n",
    "\n",
    "# Clean up output files\n",
    "snakemake clean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a dry run to see what would be executed\n",
    "try:\n",
    "    result = subprocess.run(['snakemake', '--dry-run', '--quiet'], \n",
    "                          capture_output=True, text=True, cwd='.')\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Dry run successful! Here's what would be executed:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"Dry run failed. This is expected if dependencies aren't installed.\")\n",
    "        print(\"Error:\", result.stderr[:500])\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Snakemake not installed. In a real environment, you would run:\")\n",
    "    print(\"conda install -c conda-forge snakemake\")\n",
    "    print(\"or\")\n",
    "    print(\"pip install snakemake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Snakemake Features\n",
    "\n",
    "### Configuration Files\n",
    "Keep parameters separate from code:\n",
    "\n",
    "```yaml\n",
    "# config.yaml\n",
    "scenarios:\n",
    "  baseline:\n",
    "    renewable_target: 0.3\n",
    "  high_renewable:\n",
    "    renewable_target: 0.8\n",
    "\n",
    "years: [2025, 2030, 2035]\n",
    "```\n",
    "\n",
    "### Resource Management\n",
    "Specify computational requirements:\n",
    "\n",
    "```python\n",
    "rule solve_network:\n",
    "    input: \"network.nc\"\n",
    "    output: \"results.nc\"\n",
    "    resources:\n",
    "        mem_mb=8000,    # 8GB RAM\n",
    "        runtime=60      # 1 hour max\n",
    "    script: \"solve.py\"\n",
    "```\n",
    "\n",
    "### Cluster Execution\n",
    "Scale to high-performance computing:\n",
    "\n",
    "```bash\n",
    "# Submit to SLURM cluster\n",
    "snakemake --cluster \"sbatch -t {resources.runtime} --mem={resources.mem_mb}\" \\\n",
    "          --jobs 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a Simple Example\n",
    "\n",
    "Let's create a simplified workflow to demonstrate the concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example workflow\n",
    "simple_snakefile = \"\"\"\n",
    "# Simple energy analysis workflow\n",
    "\n",
    "SCENARIOS = [\"low_renewable\", \"high_renewable\"]\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"simple_results/plot_{scenario}.png\", scenario=SCENARIOS),\n",
    "        \"simple_results/comparison.csv\"\n",
    "\n",
    "rule generate_data:\n",
    "    output:\n",
    "        \"simple_data/demand.csv\",\n",
    "        \"simple_data/supply_{scenario}.csv\"\n",
    "    params:\n",
    "        scenario=\"{scenario}\"\n",
    "    run:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import os\n",
    "        \n",
    "        os.makedirs(\"simple_data\", exist_ok=True)\n",
    "        \n",
    "        # Generate demand data\n",
    "        hours = range(24)\n",
    "        demand = 1000 + 300 * np.sin(np.array(hours) * 2 * np.pi / 24)\n",
    "        pd.DataFrame({\"hour\": hours, \"demand\": demand}).to_csv(output[0], index=False)\n",
    "        \n",
    "        # Generate supply data based on scenario\n",
    "        if params.scenario == \"low_renewable\":\n",
    "            renewable_share = 0.2\n",
    "        else:\n",
    "            renewable_share = 0.8\n",
    "            \n",
    "        renewable = renewable_share * demand * (0.5 + 0.5 * np.random.random(24))\n",
    "        fossil = demand - renewable\n",
    "        \n",
    "        pd.DataFrame({\n",
    "            \"hour\": hours, \n",
    "            \"renewable\": renewable, \n",
    "            \"fossil\": fossil\n",
    "        }).to_csv(output[1], index=False)\n",
    "\n",
    "rule create_plot:\n",
    "    input:\n",
    "        demand=\"simple_data/demand.csv\",\n",
    "        supply=\"simple_data/supply_{scenario}.csv\"\n",
    "    output:\n",
    "        \"simple_results/plot_{scenario}.png\"\n",
    "    params:\n",
    "        scenario=\"{scenario}\"\n",
    "    run:\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import os\n",
    "        \n",
    "        os.makedirs(\"simple_results\", exist_ok=True)\n",
    "        \n",
    "        demand_df = pd.read_csv(input.demand)\n",
    "        supply_df = pd.read_csv(input.supply)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.fill_between(supply_df['hour'], 0, supply_df['fossil'], \n",
    "                       label='Fossil', color='gray', alpha=0.7)\n",
    "        ax.fill_between(supply_df['hour'], supply_df['fossil'], \n",
    "                       supply_df['fossil'] + supply_df['renewable'], \n",
    "                       label='Renewable', color='green', alpha=0.7)\n",
    "        ax.plot(demand_df['hour'], demand_df['demand'], \n",
    "               'k--', linewidth=2, label='Demand')\n",
    "        \n",
    "        ax.set_xlabel('Hour of Day')\n",
    "        ax.set_ylabel('Power (MW)')\n",
    "        ax.set_title(f'Generation Mix - {params.scenario.replace(\"_\", \" \").title()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output[0], dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "rule compare_scenarios:\n",
    "    input:\n",
    "        expand(\"simple_data/supply_{scenario}.csv\", scenario=SCENARIOS)\n",
    "    output:\n",
    "        \"simple_results/comparison.csv\"\n",
    "    run:\n",
    "        import pandas as pd\n",
    "        \n",
    "        results = []\n",
    "        for i, file in enumerate(input):\n",
    "            df = pd.read_csv(file)\n",
    "            scenario = SCENARIOS[i]\n",
    "            \n",
    "            total_renewable = df['renewable'].sum()\n",
    "            total_fossil = df['fossil'].sum()\n",
    "            renewable_share = total_renewable / (total_renewable + total_fossil)\n",
    "            \n",
    "            results.append({\n",
    "                'scenario': scenario,\n",
    "                'renewable_share': renewable_share,\n",
    "                'total_renewable_mwh': total_renewable,\n",
    "                'total_fossil_mwh': total_fossil\n",
    "            })\n",
    "        \n",
    "        pd.DataFrame(results).to_csv(output[0], index=False)\n",
    "\"\"\"\n",
    "\n",
    "# Write the simple Snakefile\n",
    "with open('Snakefile_simple', 'w') as f:\n",
    "    f.write(simple_snakefile)\n",
    "\n",
    "print(\"Created simple Snakefile example!\")\n",
    "print(\"This workflow will:\")\n",
    "print(\"1. Generate synthetic demand and supply data for two scenarios\")\n",
    "print(\"2. Create plots for each scenario\")\n",
    "print(\"3. Compare the scenarios in a summary table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to run the simple workflow\n",
    "try:\n",
    "    # Clean up any existing files first\n",
    "    import shutil\n",
    "    for dir_name in ['simple_data', 'simple_results']:\n",
    "        if os.path.exists(dir_name):\n",
    "            shutil.rmtree(dir_name)\n",
    "    \n",
    "    # Run the simple workflow\n",
    "    result = subprocess.run(['snakemake', '-s', 'Snakefile_simple', '--cores', '1'], \n",
    "                          capture_output=True, text=True, cwd='.')\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Simple workflow completed successfully!\")\n",
    "        print(\"\\nOutput:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Show generated files\n",
    "        if os.path.exists('simple_results'):\n",
    "            print(\"\\nGenerated files:\")\n",
    "            for file in os.listdir('simple_results'):\n",
    "                print(f\"  - simple_results/{file}\")\n",
    "                \n",
    "        # Show comparison results\n",
    "        if os.path.exists('simple_results/comparison.csv'):\n",
    "            comparison = pd.read_csv('simple_results/comparison.csv')\n",
    "            print(\"\\nScenario Comparison:\")\n",
    "            print(comparison.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Workflow failed:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Snakemake not available. Running the workflow manually...\")\n",
    "    \n",
    "    # Execute the workflow steps manually\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('simple_data', exist_ok=True)\n",
    "    os.makedirs('simple_results', exist_ok=True)\n",
    "    \n",
    "    scenarios = [\"low_renewable\", \"high_renewable\"]\n",
    "    \n",
    "    # Generate data for each scenario\n",
    "    hours = range(24)\n",
    "    demand = 1000 + 300 * np.sin(np.array(hours) * 2 * np.pi / 24)\n",
    "    pd.DataFrame({\"hour\": hours, \"demand\": demand}).to_csv('simple_data/demand.csv', index=False)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # Generate supply data\n",
    "        renewable_share = 0.2 if scenario == \"low_renewable\" else 0.8\n",
    "        renewable = renewable_share * demand * (0.5 + 0.5 * np.random.random(24))\n",
    "        fossil = demand - renewable\n",
    "        \n",
    "        supply_df = pd.DataFrame({\n",
    "            \"hour\": hours, \n",
    "            \"renewable\": renewable, \n",
    "            \"fossil\": fossil\n",
    "        })\n",
    "        supply_df.to_csv(f'simple_data/supply_{scenario}.csv', index=False)\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.fill_between(supply_df['hour'], 0, supply_df['fossil'], \n",
    "                       label='Fossil', color='gray', alpha=0.7)\n",
    "        ax.fill_between(supply_df['hour'], supply_df['fossil'], \n",
    "                       supply_df['fossil'] + supply_df['renewable'], \n",
    "                       label='Renewable', color='green', alpha=0.7)\n",
    "        ax.plot(hours, demand, 'k--', linewidth=2, label='Demand')\n",
    "        \n",
    "        ax.set_xlabel('Hour of Day')\n",
    "        ax.set_ylabel('Power (MW)')\n",
    "        ax.set_title(f'Generation Mix - {scenario.replace(\"_\", \" \").title()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'simple_results/plot_{scenario}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_renewable = supply_df['renewable'].sum()\n",
    "        total_fossil = supply_df['fossil'].sum()\n",
    "        renewable_share_actual = total_renewable / (total_renewable + total_fossil)\n",
    "        \n",
    "        results.append({\n",
    "            'scenario': scenario,\n",
    "            'renewable_share': renewable_share_actual,\n",
    "            'total_renewable_mwh': total_renewable,\n",
    "            'total_fossil_mwh': total_fossil\n",
    "        })\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    comparison_df.to_csv('simple_results/comparison.csv', index=False)\n",
    "    \n",
    "    print(\"Manual workflow execution completed!\")\n",
    "    print(\"\\nScenario Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for Research Workflows\n",
    "\n",
    "### 1. Modular Design\n",
    "- Break complex analyses into small, focused steps\n",
    "- Each rule should have a single, clear purpose\n",
    "- Use meaningful names for rules and files\n",
    "\n",
    "### 2. Configuration Management\n",
    "```yaml\n",
    "# config.yaml - Keep all parameters in one place\n",
    "data:\n",
    "  start_year: 2020\n",
    "  end_year: 2050\n",
    "  regions: [\"north\", \"south\", \"east\", \"west\"]\n",
    "\n",
    "scenarios:\n",
    "  baseline:\n",
    "    co2_price: 0\n",
    "    renewable_target: 0.3\n",
    "  policy:\n",
    "    co2_price: 100\n",
    "    renewable_target: 0.8\n",
    "```\n",
    "\n",
    "### 3. Error Handling\n",
    "- Validate inputs in your scripts\n",
    "- Use informative error messages\n",
    "- Log intermediate results for debugging\n",
    "\n",
    "### 4. Documentation\n",
    "```python\n",
    "rule download_weather_data:\n",
    "    \"\"\"\n",
    "    Download historical weather data from ECMWF ERA5 reanalysis.\n",
    "    \n",
    "    This rule fetches wind speed and solar irradiance data for the \n",
    "    specified years and regions. The data is used to calculate \n",
    "    renewable energy potential.\n",
    "    \"\"\"\n",
    "    output:\n",
    "        \"data/raw/weather_{year}.nc\"\n",
    "    params:\n",
    "        year=\"{year}\"\n",
    "    script:\n",
    "        \"scripts/download_weather.py\"\n",
    "```\n",
    "\n",
    "### 5. Testing\n",
    "- Create test data for development\n",
    "- Test individual rules\n",
    "- Use continuous integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Integration with PyPSA\n",
    "\n",
    "Our main workflow demonstrates how to integrate Snakemake with PyPSA for comprehensive energy system analysis:\n",
    "\n",
    "### Key Integration Points:\n",
    "\n",
    "#### 1. Data Preparation\n",
    "```python\n",
    "rule prepare_network_data:\n",
    "    input:\n",
    "        buses=\"data/raw/buses.csv\",\n",
    "        lines=\"data/raw/lines.csv\",\n",
    "        generators=\"data/raw/generators.csv\"\n",
    "    output:\n",
    "        \"data/processed/network_base.nc\"\n",
    "    script:\n",
    "        \"scripts/build_pypsa_network.py\"\n",
    "```\n",
    "\n",
    "#### 2. Scenario Configuration\n",
    "```python\n",
    "rule apply_scenario:\n",
    "    input:\n",
    "        \"data/processed/network_base.nc\"\n",
    "    output:\n",
    "        \"data/networks/network_{scenario}_{year}.nc\"\n",
    "    params:\n",
    "        scenario_config=lambda w: config[\"scenarios\"][w.scenario]\n",
    "    script:\n",
    "        \"scripts/apply_scenario.py\"\n",
    "```\n",
    "\n",
    "#### 3. Parallel Optimization\n",
    "```python\n",
    "rule solve_pypsa:\n",
    "    input:\n",
    "        \"data/networks/network_{scenario}_{year}.nc\"\n",
    "    output:\n",
    "        \"results/solved/network_{scenario}_{year}.nc\"\n",
    "    resources:\n",
    "        mem_mb=16000,  # 16GB for large networks\n",
    "        runtime=120    # 2 hours max\n",
    "    script:\n",
    "        \"scripts/solve_pypsa.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Scaling to High-Performance Computing\n",
    "\n",
    "### Cluster Configuration\n",
    "\n",
    "```yaml\n",
    "# cluster.yaml - HPC job specifications\n",
    "__default__:\n",
    "  partition: \"normal\"\n",
    "  time: \"01:00:00\"\n",
    "  mem: \"4G\"\n",
    "  nodes: 1\n",
    "  ntasks: 1\n",
    "\n",
    "solve_network:\n",
    "  partition: \"bigmem\"\n",
    "  time: \"04:00:00\"\n",
    "  mem: \"32G\"\n",
    "  ntasks: 8\n",
    "```\n",
    "\n",
    "### Submission Script\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# submit_workflow.sh\n",
    "\n",
    "snakemake \\\n",
    "    --cluster-config cluster.yaml \\\n",
    "    --cluster \"sbatch -p {cluster.partition} -t {cluster.time} --mem={cluster.mem}\" \\\n",
    "    --jobs 50 \\\n",
    "    --latency-wait 60 \\\n",
    "    --rerun-incomplete\n",
    "```\n",
    "\n",
    "### Cloud Computing\n",
    "```bash\n",
    "# Run on Google Cloud with Kubernetes\n",
    "snakemake --kubernetes \\\n",
    "    --default-remote-provider GS \\\n",
    "    --default-remote-prefix gs://my-energy-bucket\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Reproducibility Features\n",
    "\n",
    "### Software Environment\n",
    "```yaml\n",
    "# conda environment for each rule\n",
    "rule solve_network:\n",
    "    input: \"network.nc\"\n",
    "    output: \"results.nc\"\n",
    "    conda: \"envs/pypsa.yaml\"\n",
    "    script: \"solve.py\"\n",
    "```\n",
    "\n",
    "### Container Support\n",
    "```python\n",
    "rule analyze_results:\n",
    "    input: \"results.nc\"\n",
    "    output: \"analysis.html\"\n",
    "    container: \"docker://pypsa/pypsa:latest\"\n",
    "    script: \"analyze.py\"\n",
    "```\n",
    "\n",
    "### Provenance Tracking\n",
    "```bash\n",
    "# Generate detailed report\n",
    "snakemake --report report.html\n",
    "\n",
    "# Archive workflow\n",
    "snakemake --archive workflow_archive.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercise: Build Your Own Workflow\n",
    "\n",
    "Create a Snakemake workflow for a simple energy analysis:\n",
    "\n",
    "### Requirements:\n",
    "1. **Data Generation**: Create synthetic hourly load data for a week\n",
    "2. **Processing**: Calculate daily and weekly statistics\n",
    "3. **Visualization**: Create plots showing:\n",
    "   - Hourly load profile\n",
    "   - Daily load distribution\n",
    "   - Weekly patterns\n",
    "4. **Analysis**: Calculate peak demand, load factor, and variability metrics\n",
    "5. **Report**: Generate a summary markdown report\n",
    "\n",
    "### Workflow Structure:\n",
    "```\n",
    "exercise_workflow/\n",
    "├── Snakefile\n",
    "├── config.yaml\n",
    "├── scripts/\n",
    "│   ├── generate_load_data.py\n",
    "│   ├── calculate_statistics.py\n",
    "│   ├── create_plots.py\n",
    "│   └── generate_report.py\n",
    "└── results/\n",
    "    ├── data/\n",
    "    ├── plots/\n",
    "    └── report.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter code\n",
    "exercise_snakefile = \"\"\"\n",
    "# Exercise: Load Analysis Workflow\n",
    "\n",
    "configfile: \"exercise_config.yaml\"\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        \"exercise_results/report.md\"\n",
    "\n",
    "rule generate_load_data:\n",
    "    output:\n",
    "        \"exercise_data/hourly_load.csv\"\n",
    "    params:\n",
    "        config=config\n",
    "    script:\n",
    "        \"exercise_scripts/generate_load_data.py\"\n",
    "\n",
    "# TODO: Add more rules for statistics, plots, and report\n",
    "\"\"\"\n",
    "\n",
    "exercise_config = \"\"\"\n",
    "# Exercise Configuration\n",
    "simulation:\n",
    "  days: 7\n",
    "  base_load: 1000  # MW\n",
    "  peak_factor: 1.5\n",
    "  noise_level: 0.1\n",
    "\n",
    "analysis:\n",
    "  metrics:\n",
    "    - \"peak_demand\"\n",
    "    - \"load_factor\"\n",
    "    - \"variability\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Exercise starter files:\")\n",
    "print(\"\\n1. Snakefile:\")\n",
    "print(exercise_snakefile)\n",
    "print(\"\\n2. Config file:\")\n",
    "print(exercise_config)\n",
    "print(\"\\nYour task: Complete the workflow by adding the missing rules and scripts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Common Pitfalls and Solutions\n",
    "\n",
    "### Problem 1: File Path Issues\n",
    "```python\n",
    "# Wrong - relative paths can be problematic\n",
    "rule bad_example:\n",
    "    script: \"../scripts/process.py\"\n",
    "\n",
    "# Better - use consistent structure\n",
    "rule good_example:\n",
    "    script: \"scripts/process.py\"\n",
    "```\n",
    "\n",
    "### Problem 2: Hardcoded Parameters\n",
    "```python\n",
    "# Wrong - parameters in code\n",
    "rule bad_parameters:\n",
    "    shell: \"python process.py --year 2025 --scenario baseline\"\n",
    "\n",
    "# Better - use config and params\n",
    "rule good_parameters:\n",
    "    params:\n",
    "        year=\"{year}\",\n",
    "        scenario=\"{scenario}\"\n",
    "    shell: \"python process.py --year {params.year} --scenario {params.scenario}\"\n",
    "```\n",
    "\n",
    "### Problem 3: Missing Dependencies\n",
    "```python\n",
    "# Wrong - missing intermediate files\n",
    "rule incomplete_deps:\n",
    "    input: \"raw_data.csv\"\n",
    "    output: \"final_result.png\"\n",
    "    # Missing intermediate processing steps!\n",
    "\n",
    "# Better - explicit dependency chain\n",
    "rule process_data:\n",
    "    input: \"raw_data.csv\"\n",
    "    output: \"processed_data.csv\"\n",
    "\n",
    "rule create_plot:\n",
    "    input: \"processed_data.csv\"\n",
    "    output: \"final_result.png\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Workflow Management**: Essential for reproducible research\n",
    "2. **Snakemake Basics**: Rules, wildcards, dependencies\n",
    "3. **Configuration**: Separating parameters from code\n",
    "4. **Parallelization**: Automatic scaling and resource management\n",
    "5. **Integration**: How to combine with PyPSA and other tools\n",
    "6. **Best Practices**: Modular design and documentation\n",
    "\n",
    "### Benefits of Workflow Management:\n",
    "\n",
    "- **Reproducibility**: Others can run your exact analysis\n",
    "- **Efficiency**: Parallel execution and smart re-running\n",
    "- **Scalability**: From laptop to supercomputer\n",
    "- **Collaboration**: Clear structure for team projects\n",
    "- **Documentation**: Workflow serves as methods documentation\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Practice**: Convert your existing analysis scripts to Snakemake\n",
    "2. **Explore**: Try different executors (cluster, cloud)\n",
    "3. **Integrate**: Combine with version control and continuous integration\n",
    "4. **Contribute**: Share workflows with the research community\n",
    "5. **Learn More**: Advanced features like checkpoints and sub-workflows\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Snakemake Documentation**: [https://snakemake.readthedocs.io/](https://snakemake.readthedocs.io/)\n",
    "- **Snakemake Tutorial**: [https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html)\n",
    "- **Energy System Workflows**: Look for published research workflows on GitHub\n",
    "- **Community**: Snakemake GitHub discussions and Stack Overflow\n",
    "\n",
    "### Final Thoughts:\n",
    "\n",
    "Workflow management is a crucial skill for modern energy research. As datasets grow larger and analyses become more complex, tools like Snakemake become indispensable for maintaining reproducible, scalable, and collaborative research practices.\n",
    "\n",
    "Start small, build incrementally, and always think about the next person (including future you) who will need to understand and run your analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}